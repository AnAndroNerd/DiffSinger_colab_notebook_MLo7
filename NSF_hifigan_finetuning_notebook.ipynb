{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO24NHrryTXb0oCwK00Jdkp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLo7Ghinsan/DiffSinger_colab_notebook_MLo7/blob/main/NSF_hifigan_finetuning_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zW_ptHQW9FXh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Mount Google Drive and Setup\n",
        "\n",
        "\n",
        "#ill put the imports here too ig\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "!rm -rf /content/sample_data\n",
        "!git clone https://github.com/openvpi/DiffSinger.git\n",
        "!git clone https://github.com/openvpi/SingingVocoders\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install click einops h5py librosa lightning matplotlib mido numpy praat-parselmouth preprocessing pyworld PyYAML torchmetrics tqdm\n",
        "#for onnx export\n",
        "!pip install onnx onnxruntime onnxsim\n",
        "!apt-get install aria2\n",
        "clear_output()\n",
        "!aria2c https://github.com/openvpi/SingingVocoders/releases/download/v0.0.1/hifi.7z\n",
        "!7z x /content/hifi.7z -o/content/SingingVocoders/pretrained/hifigan\n",
        "!rm /content/hifi.7z\n",
        "#incase theyll add it in the future lmao\n",
        "#!aria2c https://github.com/openvpi/DiffSinger/releases/download/v2.1.0/rmvpe.zip\n",
        "#!7z x /content/rmvpe.zip -o/content/SingingVocoders/pretrained\n",
        "#!rm /content/rmvpe.zip\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Extract Data\n",
        "#@markdown ___\n",
        "\n",
        "import re\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#@markdown Path to zip file containing your audio data\n",
        "data_zip_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown Segment interval in seconds\n",
        "segment_interval = 15 # @param {type:\"slider\", min:2, max:60, step:1}\n",
        "\n",
        "train_path = \"/content/audio_data/input\"\n",
        "npz_path = \"/content/audio_data/output\"\n",
        "\n",
        "\n",
        "!rm -rf /content/audio_data >/dev/null 2>&1\n",
        "\n",
        "if not os.path.exists(train_path):\n",
        "    os.makedirs(train_path)\n",
        "    os.makedirs(npz_path)\n",
        "!7z e \"$data_zip_path\" -o{train_path} \"*.wav\" -r\n",
        "\n",
        "#audio resample AND segmentation to avoid errors\n",
        "def resample_and_convert_audio(audio_path, sample_rate=44100):\n",
        "    audio, sr = librosa.load(audio_path, sr=None)\n",
        "    duration = librosa.get_duration(y=audio, sr=sr)\n",
        "\n",
        "    if sr != sample_rate:\n",
        "        audio = librosa.resample(y=audio, orig_sr=sr, target_sr=sample_rate)\n",
        "\n",
        "    if duration > segment_interval:\n",
        "        samples_per_segment = segment_interval * sample_rate\n",
        "        total_segments = int(np.ceil(duration / segment_interval))\n",
        "\n",
        "        for segment in range(total_segments):\n",
        "            start_sample = samples_per_segment * segment\n",
        "            end_sample = start_sample + samples_per_segment\n",
        "            if end_sample > len(audio):\n",
        "                end_sample = len(audio)\n",
        "            segment_audio = audio[start_sample:end_sample]\n",
        "\n",
        "            segment_filename = f\"{os.path.splitext(os.path.basename(audio_path))[0]}_segment_{segment}.wav\"\n",
        "            segment_path = os.path.join(os.path.dirname(audio_path), segment_filename)\n",
        "            sf.write(segment_path, segment_audio, sample_rate)\n",
        "        print(f\"Resampled {os.path.basename(audio_path)} to {sample_rate} Hz.\")\n",
        "        print(f\"Segmented {os.path.basename(audio_path)} into {total_segments} parts.\")\n",
        "    else:\n",
        "        sf.write(audio_path, audio, sample_rate)\n",
        "        print(f\"Resampled {os.path.basename(audio_path)} to {sample_rate} Hz.\")\n",
        "\n",
        "for root, dirs, files in os.walk(train_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            audio_path = os.path.join(root, file)\n",
        "            resample_and_convert_audio(audio_path)\n"
      ],
      "metadata": {
        "id": "WjKHanCFErik",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Edit Config\n",
        "#@markdown ___\n",
        "\n",
        "import yaml\n",
        "\n",
        "#@markdown Pitch extractor algorithm\n",
        "f0_ext = \"parselmouth\" # @param [\"parselmouth\", \"harvest\"]\n",
        "f0_min = 12 #C0\n",
        "f0_max = 2100 #about C7\n",
        "\n",
        "\n",
        "#@markdown Precision option\n",
        "precision = \"16-mixed\" # @param [\"32-true\", \"bf16-mixed\", \"16-mixed\"]\n",
        "\n",
        "#@markdown data aug option\n",
        "data_aug = False # @param {type:\"boolean\"}\n",
        "data_aug_probability = 0.5 # @param {type:\"slider\", min:0.1, max:3, step:0.1}\n",
        "\n",
        "\n",
        "#@markdown Model save interval\n",
        "save_interval = 2000 # @param {type:\"slider\", min:100, max:10000, step:100}\n",
        "save_interval = int(save_interval / 2)\n",
        "\n",
        "#@markdown Amount of validation files you want to use (can't exceed the amount of train files)\n",
        "val_amount = 8 # @param {type:\"slider\", min:1, max:18, step:1}\n",
        "\n",
        "with open(\"/content/SingingVocoders/configs/ft_hifigan.yaml\", \"r\") as config:\n",
        "    ew = yaml.safe_load(config)\n",
        "ew[\"data_input_path\"] = [\"/content/audio_data/input\"]\n",
        "ew[\"data_out_path\"] = [\"/content/audio_data/output\"]\n",
        "ew[\"val_num\"] = val_amount\n",
        "ew[\"pe\"] = f0_ext\n",
        "ew[\"f0_min\"] = f0_min\n",
        "ew[\"f0_max\"] = f0_max\n",
        "if data_aug:\n",
        "    ew[\"key_aug\"] = data_aug\n",
        "    ew[\"key_aug_prob\"] = data_aug_probability\n",
        "ew[\"val_check_interval\"] = save_interval #questionable\n",
        "ew[\"pl_trainer_accelerator\"] = \"gpu\"\n",
        "ew[\"pl_trainer_precision\"] = precision\n",
        "ew[\"finetune_ckpt_path\"] = \"/content/SingingVocoders/pretrained/hifigan/hifi.ckpt\"\n",
        "with open(\"/content/SingingVocoders/configs/ft_hifigan.yaml\", \"w\") as config:\n",
        "    yaml.dump(ew, config)\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Vs85pHx7bG7E",
        "outputId": "d3f7349b-680b-41e3-f442-a86c580eb008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Preprocess\n",
        "#@markdown ___\n",
        "\n",
        "%cd /content/SingingVocoders\n",
        "!python /content/SingingVocoders/process.py --config /content/SingingVocoders/configs/ft_hifigan.yaml --strx 1\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "NoE5hWf-l54W",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Tensorboard\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown path to your save dir basically\n",
        "logdir = \"\" # @param {type:\"string\"}\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {logdir}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JH_UodJ0QWYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SingingVocoders\n",
        "#@title # Training\n",
        "#@markdown ___\n",
        "\n",
        "config_path = \"/content/SingingVocoders/configs/ft_hifigan.yaml\" # @param {type:\"string\"}\n",
        "exp_name = \"\" # @param {type:\"string\"}\n",
        "save_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "import re\n",
        "\n",
        "training_utils_path = \"/content/SingingVocoders/utils/training_utils.py\"\n",
        "with open(training_utils_path, \"r\") as f:\n",
        "    edit_relative_path = f.read()\n",
        "new_relative = \"relative_path = filepath.relative_to(Path('/content').resolve())\"\n",
        "pattern = r\"relative_path\\s*=\\s*.*\"\n",
        "edit_relative_path = re.sub(pattern, new_relative, edit_relative_path)\n",
        "with open(training_utils_path, \"w\") as f:\n",
        "    f.write(edit_relative_path)\n",
        "\n",
        "!python /content/SingingVocoders/train.py --config {config_path} --exp_name {exp_name} --work_dir {save_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s_zvFSHo5V8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Export ONNX\n",
        "#@markdown ___\n",
        "#test exporting because the export_ckpt.py script is you know dying\n",
        "#this is a must before using diffsinger export script\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "\n",
        "#@markdown path to your vocoder ckpt\n",
        "ckpt_path = \"\" # @param {type:\"string\"}\n",
        "ckpt_folder = os.path.dirname(ckpt_path)\n",
        "ckpt_config = ckpt_folder + \"/config.yaml\"\n",
        "\n",
        "#@markdown your vocoder onnx save path\n",
        "export_path = \"\" # @param {type:\"string\"}\n",
        "save_path =  export_path + \"/model.ckpt\"\n",
        "\n",
        "#@markdown your vocoder name\n",
        "name = \"\" # @param {type:\"string\"}\n",
        "aaa2x = torch.load(ckpt_path, map_location=torch.device(\"cpu\"))['state_dict']\n",
        "ckp = {}\n",
        "for i in aaa2x:\n",
        "    i: str\n",
        "    if 'generator.' in i:\n",
        "        ckp[i.replace('generator.', '')] = aaa2x[i]\n",
        "torch.save({'generator': ckp}, save_path)\n",
        "\n",
        "# copied from openvpi's vocoder\n",
        "config_data = {\n",
        "    \"resblock\": \"1\",\n",
        "    \"num_gpus\": 4,\n",
        "    \"batch_size\": 10,\n",
        "    \"learning_rate\": 0.0002,\n",
        "    \"adam_b1\": 0.8,\n",
        "    \"adam_b2\": 0.99,\n",
        "    \"lr_decay\": 0.999,\n",
        "    \"seed\": 1234,\n",
        "    \"upsample_rates\": [8, 8, 2, 2, 2],\n",
        "    \"upsample_kernel_sizes\": [16, 16, 4, 4, 4],\n",
        "    \"upsample_initial_channel\": 512,\n",
        "    \"resblock_kernel_sizes\": [3, 7, 11],\n",
        "    \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
        "    \"discriminator_periods\": [3, 5, 7, 11, 17, 23, 37],\n",
        "    \"segment_size\": 16384,\n",
        "    \"num_mels\": 128,\n",
        "    \"num_freq\": 1025,\n",
        "    \"n_fft\": 2048,\n",
        "    \"hop_size\": 512,\n",
        "    \"win_size\": 2048,\n",
        "    \"sampling_rate\": 44100,\n",
        "    \"fmin\": 40,\n",
        "    \"fmax\": 16000,\n",
        "    \"fmax_for_loss\": None,\n",
        "    \"num_workers\": 16,\n",
        "    \"dist_config\": {\n",
        "        \"dist_backend\": \"nccl\",\n",
        "        \"dist_url\": \"tcp://localhost:54321\",\n",
        "        \"world_size\": 1\n",
        "    }\n",
        "}\n",
        "with open(f\"{export_path}/config.json\", \"w\") as json_file:\n",
        "    json.dump(config_data, json_file, indent=4)\n",
        "\n",
        "with open(ckpt_config, \"r\") as config:\n",
        "    add_vocoder_ckpt = yaml.safe_load(config)\n",
        "add_vocoder_ckpt[\"vocoder_ckpt\"] = save_path\n",
        "with open(ckpt_config, \"w\") as config:\n",
        "    yaml.dump(add_vocoder_ckpt, config)\n",
        "\n",
        "!cp {ckpt_config} {export_path}\n",
        "\n",
        "!python /content/DiffSinger/scripts/export.py nsf-hifigan --config {export_path}/config.yaml --out {export_path} --name {name}"
      ],
      "metadata": {
        "id": "NSoKhd7eE0Wq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}