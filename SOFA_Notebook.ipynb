{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLo7Ghinsan/DiffSinger_colab_notebook_MLo7/blob/main/SOFA_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8eiFkpuYMo4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Mount Drive and install Conda\n",
        "#@markdown Basically installs all necesarry things for the notebook to work <br/> **Wait until it restarts your session !**\n",
        "#@markdown Mount Drive\n",
        "from IPython.display import clear_output\n",
        "from google.colab import drive\n",
        "\n",
        "%cd /content\n",
        "drive.mount('/content/drive')\n",
        "!pip install -q condacolab\n",
        "clear_output() # rawr\n",
        "\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Main installation\n",
        "\n",
        "%cd /content\n",
        "!conda create -n aligner python=3.8 -y\n",
        "!git clone https://github.com/qiuqiao/SOFA.git\n",
        "!git clone https://github.com/UtaUtaUtau/nnsvs-db-converter #for lab conversion\n",
        "#converter stuff\n",
        "!pip install numpy scipy soundfile librosa praat-parselmouth pydub\n",
        "#SOFA stuff\n",
        "!source activate aligner; \\\n",
        "pip install -r /content/SOFA/requirements.txt\n",
        "!source activate aligner; \\\n",
        "pip install torch torchaudio praat-parselmouth onnx onnxsim scipy\n",
        "!source activate aligner; \\\n",
        "pip install ipykernel\n"
      ],
      "metadata": {
        "id": "NxGpCHv58Q38",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Extract Data\n",
        "#@markdown ___\n",
        "%cd /content\n",
        "#@markdown this cell will create a folder name [raw_data] in the root folder and extract your data into it\n",
        "\n",
        "data_type = \"full label (.lab)\" # @param [\"full label (.lab)\", \"weak label (.txt)\", \".wav only\"]\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> The path to your data zip file\n",
        "\n",
        "data_zip_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> Select this if you want to clear your raw data folder\n",
        "\n",
        "clear_data = True # @param {type:\"boolean\"}\n",
        "\n",
        "estimate_midi = False\n",
        "\n",
        "segment_length = 30\n",
        "max_silence_phoneme_amount = 60\n",
        "\n",
        "# leaving -S at 60 so max silence can be 60 seconds that exceeds the segment legnth cap idk why///\n",
        "# making the segment length cap at 35 secs because any longer than that would make training goes really slow\n",
        "\n",
        "# my ass dont remember why i made two... i think one is unnecessary extra but mehhh\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import shutil\n",
        "from pydub import AudioSegment\n",
        "\n",
        "if clear_data:\n",
        "    if os.path.exists(\"/content/SOFA/data\"):\n",
        "        shutil.rmtree(\"/content/SOFA/data\")\n",
        "\n",
        "full_lab_path = \"/content/SOFA/data/full_label\"\n",
        "weak_lab_path = \"/content/SOFA/data/weak_label\"\n",
        "no_label = \"/content/SOFA/data/no_label\"\n",
        "if not os.path.exists(full_lab_path):\n",
        "    os.makedirs(full_lab_path)\n",
        "if not os.path.exists(weak_lab_path):\n",
        "    os.makedirs(weak_lab_path)\n",
        "if not os.path.exists(no_label):\n",
        "    os.makedirs(no_label)\n",
        "\n",
        "if data_type == \"full label (.lab)\":\n",
        "    !7z x \"$data_zip_path\" -o{full_lab_path}\n",
        "\n",
        "# for funny auto dict generator lmao\n",
        "out = \"/content/SOFA/dictionary/custom_dict.txt\"\n",
        "\n",
        "phonemes = set()\n",
        "\n",
        "def is_excluded(phoneme):\n",
        "    return phoneme in [\"pau\", \"AP\", \"SP\", \"sil\"]\n",
        "\n",
        "if data_type == \"full label (.lab)\":\n",
        "    phoneme_folder_path = full_lab_path\n",
        "    for root, dirs, files in os.walk(phoneme_folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".lab\"):\n",
        "                fpath = os.path.join(root, file)\n",
        "                with open(fpath, \"r\") as lab_file:\n",
        "                    for line in lab_file:\n",
        "                        line = line.strip()\n",
        "                        if line:\n",
        "                            phoneme = line.split()[2]\n",
        "                            if not is_excluded(phoneme):\n",
        "                                phonemes.add(phoneme)\n",
        "\n",
        "with open(out, \"w\") as f:\n",
        "    for phoneme in sorted(phonemes):\n",
        "        f.write(phoneme + \"\t\" + phoneme + \"\\n\")\n",
        "\n",
        "# for vowels and consonants.txt.... well adding luquid type for uta's script\n",
        "dict_path = out\n",
        "vowel_types = {\"a\", \"i\", \"u\", \"e\", \"o\", \"N\", \"M\", \"NG\"}\n",
        "liquid_types = {\"y\", \"w\", \"l\", \"r\"} # r for english labels, it should be fine with jp too\n",
        "vowel_data = []\n",
        "consonant_data = []\n",
        "liquid_data = []\n",
        "\n",
        "with open(dict_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        phoneme, _ = line.strip().split(\"\\t\")\n",
        "        if phoneme[0] in vowel_types:\n",
        "            vowel_data.append(phoneme)\n",
        "        elif phoneme[0] in liquid_types:\n",
        "            liquid_data.append(phoneme)\n",
        "        else:\n",
        "            consonant_data.append(phoneme)\n",
        "\n",
        "vowel_data.sort()\n",
        "liquid_data.sort()\n",
        "consonant_data.sort()\n",
        "directory = os.path.dirname(dict_path)\n",
        "\n",
        "# make txt for language json file\n",
        "vowel_txt_path = os.path.join(directory, \"vowels.txt\")\n",
        "with open(vowel_txt_path, \"w\") as f:\n",
        "    f.write(\" \".join(vowel_data))\n",
        "liquid_txt_path = os.path.join(directory, \"liquids.txt\")\n",
        "with open(liquid_txt_path, \"w\") as f:\n",
        "    f.write(\" \".join(liquid_data))\n",
        "consonant_txt_path = os.path.join(directory, \"consonants.txt\")\n",
        "with open(consonant_txt_path, \"w\") as f:\n",
        "    f.write(\" \".join(consonant_data))\n",
        "\n",
        "\n",
        "# here's a funny json append\n",
        "with open(vowel_txt_path, \"r\") as f:\n",
        "    vowel_data = f.read().split()\n",
        "with open(liquid_txt_path, \"r\") as f:\n",
        "    liquid_data = f.read().split()\n",
        "with open(consonant_txt_path, \"r\") as f:\n",
        "    consonant_data = f.read().split()\n",
        "phones4json = {\"vowels\": vowel_data, \"liquids\": liquid_data}\n",
        "with open(\"/content/nnsvs-db-converter/lang.sample.json\", \"w\") as rawr:\n",
        "    json.dump(phones4json, rawr, indent=4)\n",
        "\n",
        "if data_type == \"full label (.lab)\":\n",
        "    db_converter_script = \"/content/nnsvs-db-converter/db_converter.py\"\n",
        "    for raw_folder_name in os.listdir(full_lab_path):\n",
        "        raw_folder_path = os.path.join(full_lab_path, raw_folder_name)\n",
        "        if os.path.isdir(raw_folder_path):\n",
        "            if estimate_midi:\n",
        "                !python {db_converter_script} -s {max_silence_phoneme_amount} -S 60 -l {segment_length} -m -c -L \"/content/nnsvs-db-converter/lang.sample.json\" --folder {raw_folder_path}\n",
        "            else:\n",
        "                #!python {db_converter_script} -s 2 --folder {raw_folder_path} 2> /dev/null\n",
        "                !python {db_converter_script} -s {max_silence_phoneme_amount} -S 60 -l {segment_length} -L \"/content/nnsvs-db-converter/lang.sample.json\" --folder {raw_folder_path}\n",
        "\n",
        "else:\n",
        "    pass\n",
        "\n",
        "\n",
        "if data_type == \"full label (.lab)\":\n",
        "    for raw_folder_name in os.listdir(full_lab_path):\n",
        "        raw_folder_path = os.path.join(full_lab_path, raw_folder_name)\n",
        "        !rm -rf {raw_folder_path}/*.wav {raw_folder_path}/*.lab\n",
        "        !mv {raw_folder_path}/diffsinger_db/* {raw_folder_path} 2> /dev/null\n",
        "        !rm -rf {raw_folder_path}/diffsinger_db\n",
        "\n",
        "else:\n",
        "    pass\n",
        "\n",
        "print(\"extraction complete!\")"
      ],
      "metadata": {
        "id": "C8pFl6vuZklY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Preprocess\n",
        "#@markdown ___\n",
        "%cd /content/SOFA\n",
        "\n",
        "if not os.path.exists(\"data/binary/global_config.yaml\"):\n",
        "    os.makedirs(\"data/binary\")\n",
        "    with open(\"data/binary/global_config.yaml\", \"w\") as file:\n",
        "        pass\n",
        "\n",
        "!source activate aligner; \\\n",
        "python binarize.py"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t6H3g8ipQABI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Train Your Model\n",
        "#@markdown ___\n",
        "#@markdown this is a rough draft notebook, you might have to edit the precision to 32-true in the config manually for now\n",
        "%cd /content/SOFA\n",
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=ckpt/\n",
        "\n",
        "!source activate aligner; \\\n",
        "python /content/SOFA/train.py -r"
      ],
      "metadata": {
        "id": "oFf5Mx4LHaTY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}